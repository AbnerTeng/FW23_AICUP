{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b370aa5-f4f4-445d-913b-c3900e5ed64c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def load_data(path):\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "\n",
    "def logarithm(data):\n",
    "    return data.apply(lambda x: np.log(x))\n",
    "\n",
    "\n",
    "def stacking():\n",
    "    level_0 = list()\n",
    "    level_0.append(\n",
    "        (\n",
    "            'xgb', XGBRegressor(\n",
    "                objective='reg:squarederror',\n",
    "                n_estimators=1000,\n",
    "                learning_rate=0.05,\n",
    "                max_depth=7,\n",
    "                reg_alpha= 0.05,\n",
    "                random_state=42\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    level_0.append(\n",
    "        (\n",
    "            'cat', CatBoostRegressor(\n",
    "                iterations=1000,\n",
    "                depth=10,\n",
    "                learning_rate=0.087,\n",
    "                l2_leaf_reg=0.0715564,\n",
    "                subsample=0.7963,\n",
    "                colsample_bylevel=0.94634,\n",
    "                bagging_temperature=0.0709,\n",
    "                border_count=232,\n",
    "                random_strength=0.63275,\n",
    "                verbose=False,\n",
    "                random_state=42\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    level_0.append(\n",
    "        (\n",
    "            'lgbm', LGBMRegressor(\n",
    "                num_iterations = 588,\n",
    "                learning_rate = 0.018049943310703906,\n",
    "                num_leaves = 829,\n",
    "                subsample = 0.8920214447324074,\n",
    "                colsample_bytree = 0.5330930972309851,\n",
    "                min_data_in_leaf = 25,\n",
    "                max_bin = 505,\n",
    "                random_state=42\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    level_1 = Ridge(alpha=0.5)\n",
    "    stack_model = StackingRegressor(estimators=level_0, final_estimator=level_1, cv=5)\n",
    "    return stack_model\n",
    "\n",
    "class BetaEncoder(object):\n",
    "        \n",
    "    def __init__(self, group):\n",
    "        self.group = group\n",
    "        self.stats = None\n",
    "        \n",
    "    # get counts from df\n",
    "    def fit(self, df, target_col):\n",
    "        self.prior_mean = np.mean(df[target_col]) \n",
    "        stats = df[[target_col, self.group]].groupby(self.group)\n",
    "        stats = stats.agg(['sum', 'count'])[target_col]    \n",
    "        stats.rename(columns={'sum': 'n', 'count': 'N'}, inplace=True)\n",
    "        stats.reset_index(level=0, inplace=True)           \n",
    "        self.stats = stats\n",
    "\n",
    "    # extract posterior statistics\n",
    "    def transform(self, df, stat_type, N_min=1):\n",
    "        \n",
    "        df_stats = pd.merge(df[[self.group]], self.stats, how='left')\n",
    "        n = df_stats['n'].copy()\n",
    "        N = df_stats['N'].copy()\n",
    "        \n",
    "        # fill in missing\n",
    "        nan_indexs = np.isnan(n)\n",
    "        n[nan_indexs] = self.prior_mean\n",
    "        N[nan_indexs] = 1.0\n",
    "        \n",
    "        # prior parameters\n",
    "        N_prior = np.maximum(N_min-N, 0)\n",
    "        alpha_prior = self.prior_mean*N_prior\n",
    "        beta_prior  = (1-self.prior_mean)*N_prior\n",
    "        \n",
    "        # posterior parameters\n",
    "        alpha =  alpha_prior + n\n",
    "        beta =  beta_prior  + N-n\n",
    "        \n",
    "        # calculate statistics\n",
    "        if stat_type=='mean':\n",
    "            num = alpha\n",
    "            dem = alpha+beta\n",
    "                    \n",
    "        elif stat_type=='mode':\n",
    "            num = alpha-1\n",
    "            dem = alpha+beta-2\n",
    "            \n",
    "        elif stat_type=='median':\n",
    "            num = alpha-1/3\n",
    "            dem = alpha+beta-2/3\n",
    "        \n",
    "        elif stat_type=='var':\n",
    "            num = alpha*beta\n",
    "            dem = (alpha+beta)**2*(alpha+beta+1)\n",
    "                    \n",
    "        elif stat_type=='skewness':\n",
    "            num = 2*(beta-alpha)*np.sqrt(alpha+beta+1)\n",
    "            dem = (alpha+beta+2)*np.sqrt(alpha*beta)\n",
    "\n",
    "        elif stat_type=='kurtosis':\n",
    "            num = 6*(alpha-beta)**2*(alpha+beta+1) - alpha*beta*(alpha+beta+2)\n",
    "            dem = alpha*beta*(alpha+beta+2)*(alpha+beta+3)\n",
    "            \n",
    "        # replace missing\n",
    "        value = num/dem\n",
    "        value[np.isnan(value)] = np.nanmedian(value)\n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c4abfc2-afca-49f0-b546-bdd0c875679b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# x_data = load_data(f\"{os.getcwd()}/data/train_feat.csv\")\n",
    "# y_data = load_data(f\"{os.getcwd()}/data/train_output.csv\")\n",
    "# y_train = load_data(f\"{os.getcwd()}/data/y_train.csv\")\n",
    "# y_valid = load_data(f\"{os.getcwd()}/data/y_valid.csv\")\n",
    "# public_train = load_data(f\"{os.getcwd()}/data/test_feat.csv\")\n",
    "sub_data = load_data(f\"{os.getcwd()}/data/public_submission_template.csv\")\n",
    "# # private_train = load_data(f\"{os.getcwd()}/data/test_feat_v2.csv\")\n",
    "# x_train, x_valid, y_train, y_valid = train_test_split(x_data, y_data)\n",
    "# print(\n",
    "#     f\"Train shape: {x_train.shape}, {y_train.shape}, \\\n",
    "#         Valid shape: {x_valid.shape}, {y_valid.shape}\"\n",
    "# )\n",
    "# y_train, y_valid = logarithm(y_train), logarithm(y_valid)\n",
    "# stack_model = stacking()\n",
    "# stack_model.fit(x_train, y_train)\n",
    "# y_pred = stack_model.predict(x_valid)\n",
    "# y_pred, y_valid = np.exp(y_pred), np.exp(y_valid)\n",
    "# mape = mean_absolute_percentage_error(y_valid, y_pred)\n",
    "# print(f\"MAPE: {mape * 100}\")\n",
    "# public_pred = np.exp(stack_model.predict(public_train))\n",
    "# sub_data['predicted_price'] = public_pred\n",
    "# sub_data.to_csv(f\"{os.getcwd()}/data/public_submission_stack_v4.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90b7477b-9687-4389-ac2b-8bc6e8e0e592",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv(\"./data/training_data.csv\")\n",
    "# display(raw_data)\n",
    "\n",
    "train = pd.read_csv(\"./data/train_feat.csv\")\n",
    "# display(train.info())\n",
    "feat_cols = ['土地面積','移轉層次','總樓層數','屋齡','建物面積','車位面積','車位個數','橫坐標','縱坐標','主建物面積','陽台面積',\n",
    "                '附屬建物面積','N_lib_2000','avg_distances_高中','avg_distances_國小','avg_distances_火車','avg_distances_醫療',\n",
    "                'avg_distances_公車','avg_distances_國中','avg_distances_大學','avg_distances_便利','avg_distances_AT',\n",
    "                'avg_distances_金融','avg_distances_捷運','avg_distances_郵局',\n",
    "                'avg_tax','density','edu_p']\n",
    "# '縣市_台北市','縣市_台中市','縣市_台南市','縣市_新北市','縣市_高雄市','縣市_桃園市'\n",
    "cat_cols = ['使用分區','主要用途','主要建材','建物型態','縣市']\n",
    "\n",
    "# raw_data['縣市_鄉鎮市區'] = raw_data['縣市'] + '_' + raw_data['鄉鎮市區']\n",
    "\n",
    "selected_X = train[feat_cols]\n",
    "cat_X = raw_data[cat_cols+['單價']]\n",
    "X = pd.concat([selected_X, cat_X], axis=1)\n",
    "Y = pd.read_csv(\"./data/train_output.csv\")\n",
    "Y = np.log(Y)\n",
    "\n",
    "test = pd.read_csv(\"./data/test_feat.csv\")\n",
    "raw_test_data = pd.read_csv(\"./data/public_dataset.csv\")\n",
    "# raw_test_data['縣市_鄉鎮市區'] = raw_test_data['縣市'] + '_' + raw_test_data['鄉鎮市區']\n",
    "selected_X = test[feat_cols]\n",
    "cat_X = raw_test_data[cat_cols]\n",
    "X_test = pd.concat([selected_X, cat_X], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9891c3e7-d1ce-4239-aad7-86b956bc527b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "N_min = 20\n",
    "\n",
    "for col in cat_cols:\n",
    "    # print(f\"now at {col}\")\n",
    "    le = LabelEncoder()\n",
    "    le.fit(np.concatenate([X[col], X_test[col]]))\n",
    "    X[col] = le.transform(X[col])\n",
    "    X_test[col] = le.transform(X_test[col])\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(X, Y, test_size=0.2, random_state=507)\n",
    "x_train.reset_index(inplace=True, drop=True)\n",
    "x_valid.reset_index(inplace=True, drop=True)\n",
    "y_train.reset_index(inplace=True, drop=True)\n",
    "y_valid.reset_index(inplace=True, drop=True)\n",
    "y_valid = np.exp(y_valid)\n",
    "\n",
    "\n",
    "for c in cat_cols:\n",
    "    # fit encoder\n",
    "    be = BetaEncoder(c)\n",
    "    be.fit(x_train, '單價')\n",
    "    # mean\n",
    "    feature_name = f'{c}_mean'\n",
    "    x_train[feature_name] = be.transform(x_train, 'mean', N_min)\n",
    "    x_valid[feature_name]  = be.transform(x_valid,  'mean', N_min)\n",
    "    X_test[feature_name]  = be.transform(X_test,  'mean', N_min)\n",
    "\n",
    "x_train = x_train.drop(['單價']+cat_cols,axis=1)\n",
    "x_valid = x_valid.drop(['單價']+cat_cols,axis=1)\n",
    "X_test = X_test.drop(cat_cols,axis=1)\n",
    "\n",
    "# x_train : training data x\n",
    "# y_train : training data target label. Already log\n",
    "# x_valid : validation data x\n",
    "# y_valid : validation data target label. Already exp\n",
    "# X_test : public training data x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95a4d4cb-6a97-48c0-b2b0-1ff27f6ce4af",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (9400, 33), (9400, 1),         Valid shape: (2351, 33), (2351, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kogby\\miniconda3\\envs\\Jupyter\\lib\\site-packages\\sklearn\\ensemble\\_stacking.py:955: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\kogby\\miniconda3\\envs\\Jupyter\\lib\\site-packages\\lightgbm\\engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=14, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kogby\\miniconda3\\envs\\Jupyter\\lib\\site-packages\\lightgbm\\engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
      "C:\\Users\\kogby\\miniconda3\\envs\\Jupyter\\lib\\site-packages\\lightgbm\\engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=14, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kogby\\miniconda3\\envs\\Jupyter\\lib\\site-packages\\lightgbm\\engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=14, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kogby\\miniconda3\\envs\\Jupyter\\lib\\site-packages\\lightgbm\\engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=14, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kogby\\miniconda3\\envs\\Jupyter\\lib\\site-packages\\lightgbm\\engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=14, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=14\n",
      "MAPE: 9.065768394075246\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"Train shape: {x_train.shape}, {y_train.shape}, \\\n",
    "        Valid shape: {x_valid.shape}, {y_valid.shape}\"\n",
    ")\n",
    "\n",
    "stack_model = stacking()\n",
    "stack_model.fit(x_train, y_train)\n",
    "y_pred = stack_model.predict(x_valid)\n",
    "y_pred = np.exp(y_pred)\n",
    "mape = mean_absolute_percentage_error(y_valid, y_pred)\n",
    "print(f\"MAPE: {mape * 100}\")\n",
    "public_pred = np.exp(stack_model.predict(X_test))\n",
    "sub_data['predicted_price'] = public_pred\n",
    "sub_data.to_csv(f\"{os.getcwd()}/data/public_submission_stack_v4.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c6ebe4-3f7b-4429-b486-7ceed2eb054d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
